# Docker를 이용한 Hadoop 분산 Cluster - Spark 환경과 Kafka 분산 broker환경 구축 및 연동

Hadoop은 빅데이터 처리를 HDFS라는 디스크 위에서 맵리듀스를 통해 데이터셋을 쪼개 병렬처리하게 된다
하지만 HDFS를 사용하기 위해서는 Disk I/O는 불가피한데 Disk I/O는 다들 알다시피 보조 저장장치에서 발생하기 때문에 속도의 한계가 있다
이 때 인 메모리 방식으로 더욱 빠른 속도로 데이터를 분산 처리하는 spark가 등장하여 맵리듀스를 대체하게 된다.
또한 스파크는 스파크 스트리밍이라는 실시간 데이터 처리에도 사용될 수 있어 실시간 대쉬보드에 쓰일 데이터를 처리하면서 HDFS에도 데이터를 저장할 수 있는 환경을 마련했다

카프카는 실시간으로 기록 스트림을 게시, 구독, 저장 및 처리할 수 있는 분산 데이터 스트리밍 플랫폼이다
N대의 데이터 공급자와 M대의 데이터 소비자를 연결시켜 필요한 곳에 실시간으로 공급하는 역할을 한다

현재 하나의 AWS 서버를 갖고 있고 Hadoop과 Kafka를 분산 환경에서 구동시키기 위해 docker를 사용하여 해당 환경을 구축하기로 했다

이 과정은 https://1mini2.tistory.com/99 를 참조하여 만들어졌다
